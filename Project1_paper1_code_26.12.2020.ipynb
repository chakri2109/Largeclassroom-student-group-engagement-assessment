{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv2d_1/convolution (defined at D:\\anaconda3\\envs\\pck1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_791]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-92ad8db7a242>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[0mtempimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[0mdetections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect_faces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdetection\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdetections\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pck1\\lib\\site-packages\\mtcnn\\mtcnn.py\u001b[0m in \u001b[0;36mdetect_faces\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;31m# We pipe here each of the stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mtotal_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pck1\\lib\\site-packages\\mtcnn\\mtcnn.py\u001b[0m in \u001b[0;36m__stage1\u001b[1;34m(self, image, scales, stage_status)\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[0mimg_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0mout0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pck1\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1462\u001b[1;33m                                             callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pck1\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pck1\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pck1\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pck1\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pck1\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pck1\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pck1\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pck1\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv2d_1/convolution (defined at D:\\anaconda3\\envs\\pck1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_791]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    " \n",
    "detector = MTCNN()\n",
    "\n",
    "def trim(frame):\n",
    "    #crop top\n",
    "    if not np.sum(frame[0]):\n",
    "        return trim(frame[1:])\n",
    "    #crop bottom\n",
    "    elif not np.sum(frame[-1]):\n",
    "        return trim(frame[:-2])\n",
    "    #crop left\n",
    "    elif not np.sum(frame[:,0]):\n",
    "        return trim(frame[:,1:]) \n",
    "    #crop right\n",
    "    elif not np.sum(frame[:,-1]):\n",
    "        return trim(frame[:,:-2])    \n",
    "    return frame\n",
    "\n",
    "\n",
    "def EuclideanDistance(point1,point2):\n",
    "    # finding sum of squares \n",
    "    sum_sq = np.sum(np.square(point1 - point2))\n",
    "    # Doing squareroot and \n",
    "    # printing Euclidean distance \n",
    "    return np.sqrt(sum_sq)\n",
    "\n",
    "    \n",
    "def alignment_procedure(img, left_eye, right_eye):\n",
    "#this function aligns given face in img based on left and right eye coordinates\n",
    " \n",
    "    left_eye_x, left_eye_y = left_eye\n",
    "    right_eye_x, right_eye_y = right_eye\n",
    "\n",
    "    #-----------------------\n",
    "    #find rotation direction\n",
    "\n",
    "    if left_eye_y > right_eye_y:\n",
    "        point_3rd = (right_eye_x, left_eye_y)\n",
    "        direction = -1 #rotate same direction to clock\n",
    "    else:\n",
    "        point_3rd = (left_eye_x, right_eye_y)\n",
    "        direction = 1 #rotate inverse direction of clock\n",
    "\n",
    "    #-----------------------\n",
    "    #find length of triangle edges\n",
    "\n",
    "    a = EuclideanDistance(np.array(left_eye), np.array(point_3rd))\n",
    "    b = EuclideanDistance(np.array(right_eye), np.array(point_3rd))\n",
    "    c = EuclideanDistance(np.array(right_eye), np.array(left_eye))\n",
    "\n",
    "    #-----------------------\n",
    "\n",
    "    #apply cosine rule\n",
    "\n",
    "    if b != 0 and c != 0: #this multiplication causes division by zero in cos_a calculation\n",
    "\n",
    "        cos_a = (b*b + c*c - a*a)/(2*b*c)\n",
    "        angle = np.arccos(cos_a) #angle in radian\n",
    "        angle = (angle * 180) / math.pi #radian to degree\n",
    "\n",
    "        #-----------------------\n",
    "        #rotate base image\n",
    "\n",
    "        if direction == -1:\n",
    "            angle = 90 - angle\n",
    "\n",
    "        img = Image.fromarray(img)\n",
    "        img = np.array(img.rotate(direction * angle))\n",
    "\n",
    "    #-----------------------\n",
    "\n",
    "    return img #return img anyway\n",
    "\n",
    "img = cv2.imread(\"G:\\\\Master Classroom Dataset\\\\studentsfaces____MTCNN\\\\0MTCNN5790.jpg\")\n",
    "#G:\\\\Master Classroom Dataset\\\\sample13.jpg\")\n",
    "\n",
    "tempimg=img.copy()\n",
    "detections = detector.detect_faces(img)\n",
    "     \n",
    "for detection in detections:\n",
    "    score = detection[\"confidence\"]\n",
    "    if score > 0.90:\n",
    "        x, y, w, h = detection[\"box\"]\n",
    "        x1,y1,w1,h1=x-20,y-20,w+20,h+20\n",
    "        detected_face = img[int(y):int(y+h), int(x):int(x+w)]\n",
    "        cv2.rectangle(tempimg,(x,y),(x+w,y+h),(0,0,255),2)\n",
    "        keypoints = detection[\"keypoints\"]\n",
    "        left_eye = keypoints[\"left_eye\"]\n",
    "        right_eye = keypoints[\"right_eye\"]\n",
    "\n",
    "        aligned_face=alignment_procedure(detected_face,left_eye,right_eye)\n",
    "    \n",
    "    cv2.imshow(\"image\",tempimg)\n",
    "    cv2.imshow(\"image1\",detected_face)\n",
    "    cv2.imshow(\"image2\",aligned_face)\n",
    "    cv2.imwrite(\"G:\\\\Master Classroom Dataset\\\\aligned_sample13.jpg\",aligned_face)\n",
    "    #cv2.imshow(\"image3\",trim(aligned_face))\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\pchak\\\\Phd_Implementation_Works\\\\Paper1_code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\anaconda3\\envs\\pck1\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "D:\\anaconda3\\envs\\pck1\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "D:\\anaconda3\\envs\\pck1\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loding Face detector model......\n",
      "Face detector model is loded.\n",
      "Loding Face emotion recognition model......\n",
      "Face emotion recognition model is loded.\n",
      "Frame rate 30.0\n",
      "Video segments group engagement level counter: [27, 26, 46]\n",
      "Overall class engagement level is: 2\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Dec 24 11:19:24 2020\n",
    "\n",
    "@author: pchak\n",
    "\"\"\"\n",
    "# Executing code for Project 1 Uptodate code\n",
    "\n",
    "import cv2\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numba import jit, cuda \n",
    "# import openpyxl module \n",
    "from openpyxl import Workbook,load_workbook\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "output_location='D:\\\\SGE_Project1\\\\' #Output location path\n",
    "\n",
    "def facealing(img):\n",
    "    return img\n",
    "\n",
    "def headpose(img):\n",
    "    return img\n",
    "\n",
    "    \n",
    "excelfilename='MTAPsample.xlsx'        #Excel file name\n",
    "wb = Workbook() \n",
    "sheet = wb.active \n",
    "sheet['A1']=\"Frame number\"  #total columns= 12\n",
    "sheet['B1']=\"Detected_faces\"\n",
    "sheet['C1']=\"predicted_faces\"\n",
    "sheet['D1']=\"Sleepy_E0\"\n",
    "sheet['E1']=\"Boredome_E1\"\n",
    "sheet['F1']=\"Yawn_E2\"\n",
    "sheet['G1']=\"Frustrate_E3\"\n",
    "sheet['H1']=\"Confuse_E4\"\n",
    "sheet['I1']=\"Engage_E5\"\n",
    "sheet['J1']=\"Acc_EL1(E0+E1)\"\n",
    "sheet['K1']=\"Acc_EL2(E2+E3+E4)\"\n",
    "sheet['L1']=\"Acc_EL3(E5)\" \n",
    "wb.save(output_location+excelfilename)\n",
    "excel_path=output_location+excelfilename   \n",
    "\n",
    "\n",
    "#save student emotion label data into excel\n",
    "def writedata(lst1,lst2,lst3,lst4,lst5):\n",
    "    temp_list=[0]*3\n",
    "    temp_list[0]=lst1\n",
    "    temp_list[1]=lst2\n",
    "    temp_list[2]=lst3\n",
    "    temp_list.extend(lst4)\n",
    "    temp_list.extend(lst5)\n",
    "    # To open the workbook  \n",
    "    # workbook object is created \n",
    "    wb_obj = load_workbook(excel_path) \n",
    "    sheet=wb_obj.active\n",
    "    sheet.append(temp_list)\n",
    "    wb_obj.save(excel_path)\n",
    "\n",
    "with open(output_location+'MTAPsample.csv', mode='w') as employee_file:\n",
    "    employee_writer = csv.writer(employee_file, delimiter=',',lineterminator='\\n')\n",
    "    #employee_writer.writerow(['frameno', 'EL_index'])\n",
    "    employee_writer.writerow([0, 0])\n",
    "    employee_file.close()\n",
    "\n",
    "\n",
    "def graphfile(Fno,ELId):\n",
    "    tempdata=[0]*2\n",
    "    #framercounter=0\n",
    "    #framercounter+=Fno\n",
    "    tempdata[0]=int(Fno)\n",
    "    tempdata[1]=int(ELId)\n",
    "    with open(output_location+'MTAPsample.csv','a') as appendobj:\n",
    "        append=csv.writer(appendobj,delimiter=\",\", lineterminator='\\n')\n",
    "        append.writerow(tempdata)\n",
    "        appendobj.close()\n",
    "    time.sleep(1)\n",
    "\n",
    "#Load model for face Detection\n",
    "print(\"Loding Face detector model......\")\n",
    "detector = MTCNN()\n",
    "print(\"Face detector model is loded.\")\n",
    "\n",
    "#Load model for Emotion Detection\n",
    "print(\"Loding Face emotion recognition model......\")\n",
    "model = load_model('G:\\\\000Phd Drive -Very  Important\\\\My papers publication stuff\\\\L1 Paper 1 stuff\\\\result outputs\\\\model10.h5')\n",
    "print(\"Face emotion recognition model is loded.\")\n",
    "\n",
    "#Read input video stream\n",
    "cap=cv2.VideoCapture(1)\n",
    "#cap=cv2.VideoCapture(\"G:\\\\000Phd Drive -Very  Important\\\\My papers publication stuff\\\\MTAP_LaTeX_DL_468198_240419\\\\WIN_20201231_17_51_25_Pro.mp4\")\n",
    "#cap=cv2.VideoCapture(\"G:\\\\000Phd Drive -Very  Important\\\\My papers publication stuff\\\\MTAP figures\\\\zoom_1.mp4\")\n",
    "#cap=cv2.VideoCapture(\"G:\\\\000Phd Drive -Very  Important\\\\My papers publication stuff\\\\MTAP_LaTeX_DL_468198_240419\\\\video_MTAP.mp4\")\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "# Get frame rate of video stream\n",
    "seconds = 0.3 # two frames per second get processed \n",
    "fps=cap.get(cv2.CAP_PROP_FPS) # Gets the frames per second\n",
    "print(\"Frame rate\",fps)\n",
    "multiplier = fps * seconds\n",
    "\n",
    "# Required data variables declaration\n",
    "datalist=list()\n",
    "frame_emotions=list() # list data structure for storing emotion lables of every frame\n",
    "accumulator=[0]*6 # emotion accumulator.\n",
    "EL_counter=[0]*3 #Engagement level counter.\n",
    "GEL_counter=[0]*3 #Each video segment group engagemtn level index. \n",
    "FGEL_counter=[0]*3 # list to maintain overall class group engagement level.\n",
    "\n",
    "#output video saving file into local disk\n",
    "fourcc=cv2.VideoWriter_fourcc(*'MJPG')\n",
    "op = cv2.VideoWriter(output_location+'MTAP_sample_Large.avi',fourcc,5,(width,height)) # 5fps in video #output video filename\n",
    "\n",
    "FC=0\n",
    "try:\n",
    "    while(cap.isOpened()):\n",
    "        frameId = int(round(cap.get(1))) #current frame number, rounded b/c sometimes you get frame intervals which aren't integers...this adds a little imprecision but is likely good enough\n",
    "        ref, img_frame = cap.read()\n",
    "        #frameId=frameId1.copy()\n",
    "        if frameId % multiplier == 0:\n",
    "            FC+=1\n",
    "            #print(\"Frame Number:\",frameId)\n",
    "            temp_img=img_frame.copy()\n",
    "            #print frame number on frame\n",
    "            frame_ID=\"FrameNo:\"+str(frameId)\n",
    "            \n",
    "            #frame_count=\"Frame_count\"+str(FC)\n",
    "            cv2.putText(temp_img, frame_ID, ((10), (50)), cv2.FONT_HERSHEY_DUPLEX, 2, (255,0,255), 4)\n",
    "            \n",
    "            #cv2.putText(temp_img, frame_count, ((10), (60)), cv2.FONT_HERSHEY_DUPLEX, 1, (255,255,255), 2,cv2.LINE_AA)\n",
    "            \n",
    "            #convert image from BGR2RGB which MTCNN works well\n",
    "            img_frame = cv2.cvtColor(img_frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Detection of faces using MTCNN face detection pre-trained model\n",
    "            faces = detector.detect_faces(img_frame)\n",
    "            #print(\"Detected faces:\",len(faces))\n",
    "            detected_faces=len(faces)\n",
    "            predicted_faces=0\n",
    "            \n",
    "            #processing each face from the detected faces\n",
    "            for face in faces:\n",
    "                # get face confidence\n",
    "                confidence_score=face['confidence']\n",
    "                #print(confidence_score)\n",
    "                if confidence_score >= 0.95: # to reduce false prediction occurence\n",
    "                    xf, yf, wf, hf = face['box']\n",
    "                    cv2.rectangle(temp_img,(xf-5,yf-5),(xf+wf+5,yf+hf+5),(0,0,255),2)\n",
    "                    cv2.putText(temp_img, \"Non_candidate_face\", (int(xf), int(yf)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                    continue\n",
    "                x, y, w, h = face['box']\n",
    "                # create the shape\n",
    "                cv2.rectangle(temp_img,(x-5,y-5),(x+w+5,y+h+5),(0,255,255),2)\n",
    "                img_roi=img_frame[y:y+h,x:x+w]#cropping region of interest i.e. face area from  image\n",
    "                img_roi=cv2.resize(img_roi,(48,48))\n",
    "                \n",
    "                #pre-processing\n",
    "                #1. head pose\n",
    "                pre_face1=headpose(img_roi)\n",
    "                \n",
    "                #2. face alignment\n",
    "                pre_face2=facealing(pre_face1)\n",
    "                \n",
    "                #convert into model specific structure to predict emotion label.\n",
    "                img_pixels = image.img_to_array(pre_face2)\n",
    "                img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "                img_pixels /= 255\n",
    "                \n",
    "                #Emotion Prediction, it gives probabilities for each emotion to the given input face  \n",
    "                predictions = model.predict(img_pixels)\n",
    "                \n",
    "                #count the no. of faces emotion predicted \n",
    "                predicted_faces+=1\n",
    "                \n",
    "                #find the index of predicted maximum probability emotion class \n",
    "                max_index = np.argmax(predictions)\n",
    "                accumulator[max_index]+=1\n",
    "                \n",
    "                emotions_ID = ('Sleepy', 'Boredome', 'Yawning', 'Frustrated','Confuse', 'Engage')\n",
    "                emotions = ('0','1','2','3','4','5')\n",
    "                predicted_emotion = emotions[max_index]\n",
    "                frame_emotions.extend(predicted_emotion) # write this and frame ID into excel file\n",
    "                cv2.putText(temp_img, emotions_ID[max_index], (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0,255), 4)\n",
    "            #display image\n",
    "            temp_img= cv2.resize(temp_img, (1092,720))\n",
    "            cv2.imshow(\"image\",temp_img)\n",
    "            cv2.imwrite(\"G:\\\\000Phd Drive -Very  Important\\\\My papers publication stuff\\\\MTAP figures\\\\bbb\\\\\"+str(frameId)+\".jpg\",temp_img)\n",
    "            #save frames to create video file \n",
    "            op.write(temp_img)\n",
    "            #cv2.waitKey(0)\n",
    "            #print(accumulator)\n",
    "            # EL_ counters, save in excel sheet\n",
    "            EL_counter[0]=EL_counter[0]+(accumulator[0]+accumulator[1])\n",
    "            EL_counter[1]=EL_counter[1]+(accumulator[2]+accumulator[3]+accumulator[4])\n",
    "            EL_counter[2]=EL_counter[2]+accumulator[5]\n",
    "            \n",
    "            #save data into excel file\n",
    "            writedata(frameId,detected_faces,predicted_faces,accumulator,EL_counter)\n",
    "                      \n",
    "            #print(EL_counter)\n",
    "            accumulator[:]=[0]*6 #Reset accumulator to zero\n",
    "            \n",
    "            if FC % 5 == 0: #video segment length\n",
    "                GEL_index=np.argmax(EL_counter)\n",
    "                #send data to write into csv file to get plot in line graph\n",
    "                graphfile(frameId,GEL_index)\n",
    "                #print(\"video segment group engagement index\",GEL_index)\n",
    "                FC=0\n",
    "                EL_counter[:]=[0]*3 #reset the Egagement level counters\n",
    "                GEL_counter[GEL_index]+=1 #keep track of video segments engagement index\n",
    "                #continue:\n",
    "                     \n",
    "            if cv2.waitKey(1) & 0xff == ord(\"q\"):\n",
    "                #op.write(temp_img)\n",
    "                #writedata(frameId,detected_faces,predicted_faces,accumulator,EL_counter)\n",
    "                raise #Exception(\"input video stream interrupted...!!!!\")\n",
    "                break\n",
    "    \n",
    "    cap.release()\n",
    "    #modified 26.12.2020, 5:30pm\n",
    "    op.release()\n",
    "except:\n",
    "    op.write(temp_img)\n",
    "    writedata(frameId,detected_faces,predicted_faces,accumulator,EL_counter)\n",
    "    graphfile(frameId,GEL_index)\n",
    "    print(\"Video segments group engagement level counter:\",GEL_counter)\n",
    "    FGEF=np.argmax(GEL_counter) \n",
    "    print(\"Overall class engagement level is:\",FGEF)\n",
    "    \n",
    "finally:         \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#final code for Graph 27.12.2020 1:20pm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import time\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.rcParams['animation.html'] = 'jshtml'\n",
    "\n",
    "#%matplotlib inline\n",
    "#rcParams['figure.figsize']=8,4\n",
    "\n",
    "fig = plt.figure(figsize = (6,3))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    pullData = open(\"D:\\\\SGE_Project1\\\\eng_out_Sample2_Large.csv\",\"r\").read()\n",
    "    dataArray = pullData.split('\\n')\n",
    "    xar = []\n",
    "    yar = []\n",
    "    for eachLine in dataArray:\n",
    "        if len(eachLine)>1:\n",
    "            x,y = eachLine.split(',')\n",
    "            xar.append(int(x))\n",
    "            yar.append(int(y))\n",
    "    ax1.clear()\n",
    "    ax1.plot(xar,yar)\n",
    "    \n",
    "    \n",
    "ani = animation.FuncAnimation(fig, animate, interval=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loding Face detector model......\n",
      "Face detector model is loded.\n",
      "Loding Face emotion recognition model......\n",
      "Face emotion recognition model is loded.\n",
      "Frame rate 30.0\n",
      "Video segments group engagement level counter: [0, 0, 6]\n",
      "Overall class engagement level is: 2\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Created on Thu Dec 24 11:19:24 2020\n",
    "\n",
    "@author: pchak\n",
    "\"\"\"\n",
    "import cv2\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numba import jit, cuda \n",
    "# import openpyxl module \n",
    "from openpyxl import Workbook,load_workbook\n",
    "import csv\n",
    "import time\n",
    "\n",
    "output_location='D:\\\\SGE_Project1\\\\' #Output location path\n",
    "\n",
    "def facealing(img):\n",
    "    return img\n",
    "\n",
    "def headpose(img):\n",
    "    return img\n",
    "\n",
    "excelfilename='Sample2_Large.xlsx'        #Excel file name\n",
    "wb = Workbook() \n",
    "sheet = wb.active \n",
    "sheet['A1']=\"Frame number\"  #total columns= 12\n",
    "sheet['B1']=\"Detected_faces\"\n",
    "sheet['C1']=\"predicted_faces\"\n",
    "sheet['D1']=\"Sleepy_E0\"\n",
    "sheet['E1']=\"Boredome_E1\"\n",
    "sheet['F1']=\"Yawn_E2\"\n",
    "sheet['G1']=\"Frustrate_E3\"\n",
    "sheet['H1']=\"Confuse_E4\"\n",
    "sheet['I1']=\"Engage_E5\"\n",
    "sheet['J1']=\"Acc_EL1(E0+E1)\"\n",
    "sheet['K1']=\"Acc_EL2(E2+E3+E4)\"\n",
    "sheet['L1']=\"Acc_EL3(E5)\" \n",
    "wb.save(output_location+excelfilename)\n",
    "excel_path=output_location+excelfilename   \n",
    "\n",
    "\n",
    "#save student emotion label data into excel\n",
    "def writedata(lst1,lst2,lst3,lst4,lst5):\n",
    "    temp_list=[0]*3\n",
    "    temp_list[0]=lst1\n",
    "    temp_list[1]=lst2\n",
    "    temp_list[2]=lst3\n",
    "    temp_list.extend(lst4)\n",
    "    temp_list.extend(lst5)\n",
    "    # To open the workbook  \n",
    "    # workbook object is created \n",
    "    wb_obj = load_workbook(excel_path) \n",
    "    sheet=wb_obj.active\n",
    "    sheet.append(temp_list)\n",
    "    wb_obj.save(excel_path)\n",
    "\n",
    "with open(output_location+'eng_out_Sample2_Large.csv', mode='w') as employee_file:\n",
    "    employee_writer = csv.writer(employee_file, delimiter=',',lineterminator='\\n')\n",
    "    employee_writer.writerow([0, 0])\n",
    "    employee_file.close()\n",
    "\n",
    "\n",
    "def graphfile(Fno,ELId):\n",
    "    tempdata=[0]*2\n",
    "    #framercounter=0\n",
    "    #framercounter+=Fno\n",
    "    tempdata[0]=Fno\n",
    "    tempdata[1]=ELId\n",
    "    with open(output_location+'eng_out_Sample2_Large.csv','a') as appendobj:\n",
    "        append=csv.writer(appendobj,delimiter=\",\", lineterminator='\\n')\n",
    "        append.writerow(tempdata)\n",
    "        #appendobj.save()\n",
    "        appendobj.close()\n",
    "    time.sleep(1)\n",
    "\n",
    "#Load model for face Detection\n",
    "print(\"Loding Face detector model......\")\n",
    "detector = MTCNN()\n",
    "#print(\"Face detector model is loded.\")\n",
    "\n",
    "#Load model for Emotion Detection\n",
    "print(\"Loding Face emotion recognition model......\")\n",
    "model = load_model('G:\\\\000Phd Drive -Very  Important\\\\My papers publication stuff\\\\L1 Paper 1 stuff\\\\result outputs\\\\model10.h5')\n",
    "#print(\"Face emotion recognition model is loded.\")\n",
    "\n",
    "#Read input video stream\n",
    "cap=cv2.VideoCapture('D:\\\\SGE_Project1\\\\Sample2_Large.mp4')\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "# Get frame rate of video stream\n",
    "seconds = 0.3 # two frames per second get processed \n",
    "fps=cap.get(cv2.CAP_PROP_FPS) # Gets the frames per second\n",
    "print(\"Frame rate\",fps)\n",
    "multiplier = fps * seconds\n",
    "\n",
    "# Required data variables declaration\n",
    "datalist=list()\n",
    "frame_emotions=list() # list data structure for storing emotion lables of every frame\n",
    "accumulator=[0]*6 # emotion accumulator.\n",
    "EL_counter=[0]*3 #Engagement level counter.\n",
    "GEL_counter=[0]*3 #Each video segment group engagemtn level index. \n",
    "FGEL_counter=[0]*3 # list to maintain overall class group engagement level.\n",
    "\n",
    "#output video saving file into local disk\n",
    "fourcc=cv2.VideoWriter_fourcc(*'MJPG')\n",
    "op = cv2.VideoWriter(output_location+'test_Sample2_Large.avi',fourcc,5,(width,height)) # 5fps in video #output video filename\n",
    "\n",
    "FC=0\n",
    "try:\n",
    "    while(cap.isOpened()):\n",
    "        frameId = int(round(cap.get(1))) #current frame number, rounded b/c sometimes you get frame intervals which aren't integers...this adds a little imprecision but is likely good enough\n",
    "        ref, img_frame = cap.read()\n",
    "        #frameId=frameId1.copy()\n",
    "        if frameId % multiplier == 0:\n",
    "            FC+=1\n",
    "            #print(\"Frame Number:\",frameId)\n",
    "            temp_img=img_frame.copy()\n",
    "            #print frame number on frame\n",
    "            frame_ID=\"FrameNo:\"+str(frameId)\n",
    "            \n",
    "            #frame_count=\"Frame_count\"+str(FC)\n",
    "            cv2.putText(temp_img, frame_ID, ((10), (50)), cv2.FONT_HERSHEY_DUPLEX, 1, (255,0,255), 2)\n",
    "            \n",
    "            #cv2.putText(temp_img, frame_count, ((10), (60)), cv2.FONT_HERSHEY_DUPLEX, 1, (255,255,255), 2,cv2.LINE_AA)\n",
    "            \n",
    "            #convert image from BGR2RGB which MTCNN works well\n",
    "            img_frame = cv2.cvtColor(img_frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Detection of faces using MTCNN face detection pre-trained model\n",
    "            faces = detector.detect_faces(img_frame)\n",
    "            #print(\"Detected faces:\",len(faces))\n",
    "            detected_faces=len(faces)\n",
    "            predicted_faces=0\n",
    "            \n",
    "            #processing each face from the detected faces\n",
    "            for face in faces:\n",
    "                # get face confidence\n",
    "                confidence_score=face['confidence']\n",
    "                #print(confidence_score)\n",
    "                if confidence_score <= 0.95: # to reduce false prediction occurence\n",
    "                    xf, yf, wf, hf = face['box']\n",
    "                    cv2.rectangle(temp_img,(xf-5,yf-5),(xf+wf+5,yf+hf+5),(0,0,255),2)\n",
    "                    cv2.putText(temp_img, \"Non_candidate_face\", (int(xf), int(yf)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                    continue\n",
    "                x, y, w, h = face['box']\n",
    "                # create the shape\n",
    "                cv2.rectangle(temp_img,(x-5,y-5),(x+w+5,y+h+5),(0,255,255),2)\n",
    "                img_roi=img_frame[y:y+h,x:x+w]#cropping region of interest i.e. face area from  image\n",
    "                img_roi=cv2.resize(img_roi,(48,48))\n",
    "                \n",
    "                #pre-processing\n",
    "                #1. head pose\n",
    "                pre_face1=headpose(img_roi)\n",
    "                \n",
    "                #2. face alignment\n",
    "                pre_face2=facealing(pre_face1)\n",
    "                \n",
    "                #convert into model specific structure to predict emotion label.\n",
    "                img_pixels = image.img_to_array(pre_face2)\n",
    "                img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "                img_pixels /= 255\n",
    "                \n",
    "                #Emotion Prediction, it gives probabilities for each emotion to the given input face  \n",
    "                predictions = model.predict(img_pixels)\n",
    "                \n",
    "                #count the no. of faces emotion predicted \n",
    "                predicted_faces+=1\n",
    "                \n",
    "                #find the index of predicted maximum probability emotion class \n",
    "                max_index = np.argmax(predictions)\n",
    "                accumulator[max_index]+=1\n",
    "                \n",
    "                emotions_ID = ('Sleepy', 'Boredome', 'Yawning', 'Frustrated','Confuse', 'Engage')\n",
    "                emotions = ('0','1','2','3','4','5')\n",
    "                predicted_emotion = emotions[max_index]\n",
    "                frame_emotions.extend(predicted_emotion) # write this and frame ID into excel file\n",
    "                cv2.putText(temp_img, emotions_ID[max_index], (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            #display image\n",
    "            cv2.imshow(\"image\",temp_img)\n",
    "            #save frames to create video file \n",
    "            op.write(temp_img)\n",
    "            #cv2.waitKey(0)\n",
    "            #print(accumulator)\n",
    "            # EL_ counters, save in excel sheet\n",
    "            EL_counter[0]=EL_counter[0]+(accumulator[0]+accumulator[1])\n",
    "            EL_counter[1]=EL_counter[1]+(accumulator[2]+accumulator[3]+accumulator[4])\n",
    "            EL_counter[2]=EL_counter[2]+accumulator[5]\n",
    "            \n",
    "            #save data into excel file\n",
    "            writedata(frameId,detected_faces,predicted_faces,accumulator,EL_counter)\n",
    "                      \n",
    "            #print(EL_counter)\n",
    "            accumulator[:]=[0]*6 #Reset accumulator to zero\n",
    "            \n",
    "            if FC % 5 == 0: #video segment length\n",
    "                GEL_index=np.argmax(EL_counter)\n",
    "                #send data to write into csv file to get plot in line graph\n",
    "                graphfile(frameId,GEL_index)\n",
    "                #print(\"video segment group engagement index\",GEL_index)\n",
    "                FC=0\n",
    "                EL_counter[:]=[0]*3 #reset the Egagement level counters\n",
    "                GEL_counter[GEL_index]+=1 #keep track of video segments engagement index\n",
    "                #continue:\n",
    "            \n",
    "            \n",
    "            if cv2.waitKey(1) & 0xff == ord(\"q\"):\n",
    "                #op.write(temp_img)\n",
    "                #writedata(frameId,detected_faces,predicted_faces,accumulator,EL_counter)\n",
    "                raise #Exception(\"input video stream interrupted...!!!!\")\n",
    "                break\n",
    "    \n",
    "    cap.release()\n",
    "    #modified 26.12.2020, 5:30pm\n",
    "    op.release()\n",
    "except:\n",
    "    op.write(temp_img)\n",
    "    writedata(frameId,detected_faces,predicted_faces,accumulator,EL_counter)\n",
    "    graphfile(frameId,GEL_index)\n",
    "    print(\"Video segments group engagement level counter:\",GEL_counter)\n",
    "    FGEF=np.argmax(GEL_counter) \n",
    "    print(\"Overall class engagement level is:\",FGEF)\n",
    "    \n",
    "finally:         \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Dec 24 11:19:24 2020\n",
    "\n",
    "@author: pchak\n",
    "\"\"\"\n",
    "import cv2\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from numba import jit, cuda \n",
    "# import openpyxl module \n",
    "from openpyxl import Workbook,load_workbook\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import style\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.rcParams['animation.html'] = 'jshtml'\n",
    "\n",
    "\n",
    "output_location='D:\\\\SGE_Project1\\\\' #Output location path\n",
    "\n",
    "def facealing(img):\n",
    "    return img\n",
    "\n",
    "def headpose(img):\n",
    "    return img\n",
    "\n",
    "excelfilename='Sample2_Large.xlsx'        #Excel file name\n",
    "wb = Workbook() \n",
    "sheet = wb.active \n",
    "sheet['A1']=\"Frame number\"  #total columns= 12\n",
    "sheet['B1']=\"Detected_faces\"\n",
    "sheet['C1']=\"predicted_faces\"\n",
    "sheet['D1']=\"Sleepy_E0\"\n",
    "sheet['E1']=\"Boredome_E1\"\n",
    "sheet['F1']=\"Yawn_E2\"\n",
    "sheet['G1']=\"Frustrate_E3\"\n",
    "sheet['H1']=\"Confuse_E4\"\n",
    "sheet['I1']=\"Engage_E5\"\n",
    "sheet['J1']=\"Acc_EL1(E0+E1)\"\n",
    "sheet['K1']=\"Acc_EL2(E2+E3+E4)\"\n",
    "sheet['L1']=\"Acc_EL3(E5)\" \n",
    "wb.save(output_location+excelfilename)\n",
    "excel_path=output_location+excelfilename   \n",
    "\n",
    "\n",
    "#save student emotion label data into excel\n",
    "def writedata(lst1,lst2,lst3,lst4,lst5):\n",
    "    temp_list=[0]*3\n",
    "    temp_list[0]=lst1\n",
    "    temp_list[1]=lst2\n",
    "    temp_list[2]=lst3\n",
    "    temp_list.extend(lst4)\n",
    "    temp_list.extend(lst5)\n",
    "    # To open the workbook  \n",
    "    # workbook object is created \n",
    "    wb_obj = load_workbook(excel_path) \n",
    "    sheet=wb_obj.active\n",
    "    sheet.append(temp_list)\n",
    "    wb_obj.save(excel_path)\n",
    "\n",
    "def graphfile(Fno,ELId):\n",
    "    tempdata=[0]*2\n",
    "    #framercounter=0\n",
    "    #framercounter+=Fno\n",
    "    tempdata[0]=Fno\n",
    "    tempdata[1]=ELId\n",
    "    with open(output_location+'eng_out_Sample2_Large.csv','a') as appendobj:\n",
    "        append=csv.writer(appendobj,delimiter=\",\", lineterminator='\\n')\n",
    "        append.writerow(tempdata)\n",
    "        appendobj.close()\n",
    "    \n",
    "    \n",
    "#Load model for face Detection\n",
    "print(\"Loding Face detector model......\")\n",
    "detector = MTCNN()\n",
    "print(\"Face detector model is loded.\")\n",
    "\n",
    "#Load model for Emotion Detection\n",
    "print(\"Loding Face emotion recognition model......\")\n",
    "model = load_model('G:\\\\000Phd Drive -Very  Important\\\\My papers publication stuff\\\\L1 Paper 1 stuff\\\\result outputs\\\\model10.h5')\n",
    "print(\"Face emotion recognition model is loded.\")\n",
    "\n",
    "#Read input video stream\n",
    "cap=cv2.VideoCapture('D:\\\\SGE_Project1\\\\Sample2_Large.mp4')\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "# Get frame rate of video stream\n",
    "seconds = 0.3 # two frames per second get processed \n",
    "fps=cap.get(cv2.CAP_PROP_FPS) # Gets the frames per second\n",
    "print(\"Frame rate\",fps)\n",
    "multiplier = fps * seconds\n",
    "\n",
    "# Required data variables declaration\n",
    "datalist=list()\n",
    "frame_emotions=list() # list data structure for storing emotion lables of every frame\n",
    "accumulator=[0]*6 # emotion accumulator.\n",
    "EL_counter=[0]*3 #Engagement level counter.\n",
    "GEL_counter=[0]*3 #Each video segment group engagemtn level index. \n",
    "FGEL_counter=[0]*3 # list to maintain overall class group engagement level.\n",
    "\n",
    "#output video saving file into local disk\n",
    "fourcc=cv2.VideoWriter_fourcc(*'MJPG')\n",
    "op = cv2.VideoWriter(output_location+'test_Sample2_Large.avi',fourcc,5,(width,height)) # 5fps in video #output video filename\n",
    "\n",
    "FC=0\n",
    "try:\n",
    "    while(cap.isOpened()):\n",
    "        frameId = int(round(cap.get(1))) #current frame number, rounded b/c sometimes you get frame intervals which aren't integers...this adds a little imprecision but is likely good enough\n",
    "        ref, img_frame = cap.read()\n",
    "        #frameId=frameId1.copy()\n",
    "        if frameId % multiplier == 0:\n",
    "            FC+=1\n",
    "            #print(\"Frame Number:\",frameId)\n",
    "            temp_img=img_frame.copy()\n",
    "            #print frame number on frame\n",
    "            frame_ID=\"FrameNo:\"+str(frameId)\n",
    "            \n",
    "            #frame_count=\"Frame_count\"+str(FC)\n",
    "            cv2.putText(temp_img, frame_ID, ((10), (50)), cv2.FONT_HERSHEY_DUPLEX, 1, (255,0,255), 2)\n",
    "            \n",
    "            #cv2.putText(temp_img, frame_count, ((10), (60)), cv2.FONT_HERSHEY_DUPLEX, 1, (255,255,255), 2,cv2.LINE_AA)\n",
    "            \n",
    "            #convert image from BGR2RGB which MTCNN works well\n",
    "            img_frame = cv2.cvtColor(img_frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Detection of faces using MTCNN face detection pre-trained model\n",
    "            faces = detector.detect_faces(img_frame)\n",
    "            #print(\"Detected faces:\",len(faces))\n",
    "            detected_faces=len(faces)\n",
    "            predicted_faces=0\n",
    "            \n",
    "            #processing each face from the detected faces\n",
    "            for face in faces:\n",
    "                # get face confidence\n",
    "                confidence_score=face['confidence']\n",
    "                #print(confidence_score)\n",
    "                if confidence_score <= 0.95: # to reduce false prediction occurence\n",
    "                    xf, yf, wf, hf = face['box']\n",
    "                    cv2.rectangle(temp_img,(xf-5,yf-5),(xf+wf+5,yf+hf+5),(0,0,255),2)\n",
    "                    cv2.putText(temp_img, \"Non_candidate_face\", (int(xf), int(yf)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                    continue\n",
    "                x, y, w, h = face['box']\n",
    "                # create the shape\n",
    "                cv2.rectangle(temp_img,(x-5,y-5),(x+w+5,y+h+5),(0,255,255),2)\n",
    "                img_roi=img_frame[y:y+h,x:x+w]#cropping region of interest i.e. face area from  image\n",
    "                img_roi=cv2.resize(img_roi,(48,48))\n",
    "                \n",
    "                #pre-processing\n",
    "                #1. head pose\n",
    "                pre_face1=headpose(img_roi)\n",
    "                \n",
    "                #2. face alignment\n",
    "                pre_face2=facealing(pre_face1)\n",
    "                \n",
    "                #convert into model specific structure to predict emotion label.\n",
    "                img_pixels = image.img_to_array(pre_face2)\n",
    "                img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "                img_pixels /= 255\n",
    "                \n",
    "                #Emotion Prediction, it gives probabilities for each emotion to the given input face  \n",
    "                predictions = model.predict(img_pixels)\n",
    "                \n",
    "                #count the no. of faces emotion predicted \n",
    "                predicted_faces+=1\n",
    "                \n",
    "                #find the index of predicted maximum probability emotion class \n",
    "                max_index = np.argmax(predictions)\n",
    "                accumulator[max_index]+=1\n",
    "                \n",
    "                emotions_ID = ('Sleepy', 'Boredome', 'Yawning', 'Frustrated','Confuse', 'Engage')\n",
    "                emotions = ('0','1','2','3','4','5')\n",
    "                predicted_emotion = emotions[max_index]\n",
    "                frame_emotions.extend(predicted_emotion) # write this and frame ID into excel file\n",
    "                cv2.putText(temp_img, emotions_ID[max_index], (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            #display image\n",
    "            cv2.imshow(\"image\",temp_img)\n",
    "            #save frames to create video file \n",
    "            op.write(temp_img)\n",
    "            #cv2.waitKey(0)\n",
    "            #print(accumulator)\n",
    "            # EL_ counters, save in excel sheet\n",
    "            EL_counter[0]=EL_counter[0]+(accumulator[0]+accumulator[1])\n",
    "            EL_counter[1]=EL_counter[1]+(accumulator[2]+accumulator[3]+accumulator[4])\n",
    "            EL_counter[2]=EL_counter[2]+accumulator[5]\n",
    "            \n",
    "            #save data into excel file\n",
    "            writedata(frameId,detected_faces,predicted_faces,accumulator,EL_counter)\n",
    "                      \n",
    "            #print(EL_counter)\n",
    "            accumulator[:]=[0]*6 #Reset accumulator to zero\n",
    "            \n",
    "            if FC % 5 == 0: #video segment length\n",
    "                GEL_index=np.argmax(EL_counter)\n",
    "                #send data to write into csv file to get plot in line graph\n",
    "                graphfile(frameId,GEL_index)\n",
    "                #print(\"video segment group engagement index\",GEL_index)\n",
    "                FC=0\n",
    "                EL_counter[:]=[0]*3 #reset the Egagement level counters\n",
    "                GEL_counter[GEL_index]+=1 #keep track of video segments engagement index        \n",
    "                #continue:\n",
    "            if cv2.waitKey(1) & 0xff == ord(\"q\"):\n",
    "                #op.write(temp_img)\n",
    "                #writedata(frameId,detected_faces,predicted_faces,accumulator,EL_counter)\n",
    "                raise #Exception(\"input video stream interrupted...!!!!\")\n",
    "                break\n",
    "    cap.release()\n",
    "    #modified 26.12.2020, 5:30pm\n",
    "    op.release()\n",
    "except:\n",
    "    op.write(temp_img)\n",
    "    writedata(frameId,detected_faces,predicted_faces,accumulator,EL_counter)\n",
    "    graphfile(frameId,GEL_index)\n",
    "    print(\"Video segments group engagement level counter:\",GEL_counter)\n",
    "    FGEF=np.argmax(GEL_counter) \n",
    "    print(\"Overall class engagement level is:\",FGEF)\n",
    "    \n",
    "finally:         \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
